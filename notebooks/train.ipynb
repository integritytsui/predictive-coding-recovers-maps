{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76842de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content\n",
    "\n",
    "# download the predictive coding repository\n",
    "!git clone https://github.com/jgornet/predictive-coding-recovers-maps.git\n",
    "%cd predictive-coding-recovers-maps/notebooks\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd85733",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pushd ../weights\n",
    "!python download.py\n",
    "%popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a55e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "\n",
    "from predictive_coding.dataset import collate_fn, EnvironmentDataset\n",
    "from predictive_coding.trainer import Trainer\n",
    "from predictive_coding import Autoencoder, PredictiveCoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec3f260",
   "metadata": {},
   "source": [
    "# Training a predictive coding neural network\n",
    "\n",
    "In this Google Colab notebook, we detail the procedure for training a predictive coding neural network using a dataset derived from a Minecraft environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b020665",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "The dataset captures sequences of an agent's movements within the Minecraft environment.\n",
    "Before using this data, certain preprocessing steps might be necessary, such as normalization or reshaping, depending on the nature and format of the data. It's crucial to divide the dataset into two distinct sets. The training set is used to adjust the model's weights, while the validation set helps evaluate the model's performance on unseen data and prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58a8f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EnvironmentDataset(Path(\"../datasets/train-dataset\"))\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_dataset = EnvironmentDataset(Path(\"../datasets/val-dataset\"))\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=100,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d70d18",
   "metadata": {},
   "source": [
    "# Neural Network and Optimizer Setup\n",
    "\n",
    "The predictive coding neural network is designed to forecast future states based on the current and past states. This makes it suitable for understanding sequences like our Minecraft data. An optimizer aids in updating the model's weights. Common choices include Adam, SGD, and RMSprop. The optimizer's role is to minimize the error between the predicted and actual outcomes, adjusting the model's weights in the process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6075a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'predictive-coding'\n",
    "model = PredictiveCoder(in_channels=3, out_channels=3, layers=[2, 2, 2, 2], seq_len=20)\n",
    "model = model.to('cuda:0')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0.9, weight_decay=5e-6)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-1, epochs=200, steps_per_epoch=len(train_dataloader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8995005",
   "metadata": {},
   "source": [
    "## Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91679218",
   "metadata": {},
   "source": [
    "Rather than feeding the entire dataset at once, it's broken into smaller chunks or batches. This makes the training process more manageable and often leads to better convergence.\n",
    "After each batch is processed, the model's prediction is compared to the actual outcome. Based on this comparison, the optimizer adjusts the neural network's weights to reduce prediction error.\n",
    "Iterations: This batch processing and weight adjustment are repeated multiple times (epochs) until the model's performance plateaus or meets a predetermined criterion.\n",
    "By following these steps meticulously, you can efficiently train your predictive coding neural network on the Minecraft dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d777c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = os.path.abspath('./experiments/' + experiment_name)\n",
    "if not os.path.exists(ckpt_path):\n",
    "    os.makedirs(ckpt_path, exist_ok=True)\n",
    "trainer = Trainer(model, optimizer, scheduler, train_dataloader, val_dataloader,\n",
    "                  checkpoint_path=ckpt_path)\n",
    "trainer.fit(num_epochs=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15dedc5",
   "metadata": {},
   "source": [
    "### The circular environment\n",
    "\n",
    "In this section of our Google Colab notebook, we shift our focus to the implementation of a predictive coder neural network within the circular corridor environment in Minecraft. Unlike the autoencoder, the predictive coder's primary objective is to forecast future images based on a sequence of past observations. Leveraging the temporal dependencies inherent in the environment, the network learns to anticipate the next frame in the sequence, effectively predicting how the scene will evolve over time. Similar to the autoencoder, we employ gradient descent to optimize the predictive coder's parameters, allowing it to progressively refine its predictions and generate increasingly accurate forecasts of future images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740c7d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CircleDataset(\n",
    "    '../datasets/circle-dataset/circle_images.npy', \n",
    "    '../datasets/circle-dataset/circle_positions.npy', \n",
    "    length=30, \n",
    "    speed=5\n",
    ")\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f68a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'predictive-coding-circle'\n",
    "model = PredictiveCoder(in_channels=3, out_channels=3, layers=[2, 2, 2, 2], seq_len=30)\n",
    "model = model.to('cuda:0')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5e-2, momentum=0.9, weight_decay=5e-6)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=5e-2, epochs=400, steps_per_epoch=len(train_dataloader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef361371",
   "metadata": {},
   "source": [
    "# Training an autoencoder\n",
    "\n",
    "We focus on training an autoencoder using a dataset derived from a Minecraft environment. The dataset captures attributes from an agent's movements within Minecraft and may require preprocessing, such as normalization and reshaping, to ensure compatibility with the autoencoder structure. After dividing the data into training and validation segments, the autoencoder's architecture—comprising an encoder that compresses the input and a decoder that reconstructs it—is set up. The aim is to minimize the difference between the original data and its reconstruction. An optimizer, like Adam or SGD, is employed to refine the model's weights based on the observed reconstruction error. Throughout the training phase, the model processes data in batches, using feedback from each batch to adjust its weights and improve the fidelity of the data reconstruction. This iterative process continues until the model achieves satisfactory performance or until the improvement plateau is reached.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0963ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'autoencoder'\n",
    "model = Autoencoder(in_channels=3, out_channels=3, layers=[2, 2, 2, 2])\n",
    "model = model.to('cuda:0')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0.9, weight_decay=5e-6)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-1, epochs=200, steps_per_epoch=len(train_dataloader))\n",
    "\n",
    "ckpt_path = os.path.abspath('./experiments/' + experiment_name)\n",
    "if not os.path.exists(ckpt_path):\n",
    "    os.makedirs(ckpt_path, exist_ok=True)\n",
    "trainer = Trainer(model, optimizer, scheduler, train_dataloader, val_dataloader,\n",
    "                  checkpoint_path=ckpt_path)\n",
    "trainer.fit(num_epochs=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8de4ed",
   "metadata": {},
   "source": [
    "### The circular environment\n",
    "\n",
    "In this section of our Google Colab notebook, we delve into the practical implementation of training an autoencoder neural network within the circular corridor environment in Minecraft. Mirroring our approach in the prior environment, the autoencoder's primary objective is to minimize the discrepancy between the original input image and its reconstructed counterpart. To achieve this, we employ the standard gradient descent algorithm, iteratively updating the autoencoder's parameters using the available training data. This process allows the network to progressively learn the most salient features of the environment, ultimately resulting in a model capable of generating accurate reconstructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1a85b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CircleDataset(\n",
    "    '../datasets/circle-dataset/circle_images.npy', \n",
    "    '../datasets/circle-dataset/circle_positions.npy', \n",
    "    length=30, speed=5\n",
    ")\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9c6a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'autoencoder-circle'\n",
    "model = Autoencoder(in_channels=3, out_channels=3, layers=[2, 2, 2, 2])\n",
    "model = model.to('cuda:0')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, weight_decay=5e-6)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-2, epochs=200, steps_per_epoch=len(train_dataloader))\n",
    "\n",
    "ckpt_path = os.path.abspath('./experiments/' + experiment_name)\n",
    "if not os.path.exists(ckpt_path):\n",
    "    os.makedirs(ckpt_path, exist_ok=True)\n",
    "trainer = Trainer(model, optimizer, scheduler, train_dataloader, val_dataloader,\n",
    "                  checkpoint_path=ckpt_path)\n",
    "trainer.fit(num_epochs=200)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
